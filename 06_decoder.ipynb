{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ni, vocab):\n",
    "        self.linear = nn.Linear(ni, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return nn.functional.softmax(x, dim=-1)\n",
    "\n",
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_hidden, act = nn.ReLU):\n",
    "        self.layers = nn.Sequential(nn.Linear(d_model, n_hidden), act(), nn.Linear(n_hidden, d_model))\n",
    "    def forward(self, x): return self.layers(x)\n",
    "\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, batch_first = True):\n",
    "        super().__init__()\n",
    "        self.d_model, self.num_heads = d_model, num_heads\n",
    "        self.qk_proj = nn.Linear(d_model, 2 * d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.attention_layer = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=batch_first)\n",
    "\n",
    "    def forward(self, encoder_output, v_output, mask):\n",
    "        qk = self.qk_proj(encoder_output)\n",
    "\n",
    "        # retrieve the individual matrices\n",
    "        q,k = qk.chunk(2, dim=-1)\n",
    "        v = self.v_proj(v_output)\n",
    "\n",
    "        # grab the attention output\n",
    "        output, _ = self.attention_layer(query=q, key=k, value=v, attn_mask=mask)\n",
    "        return output\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, seq_len, embed_dim, num_heads, batch_first = True):\n",
    "\n",
    "        # our first attention layer will take in our output and predicted words\n",
    "        self.qkv_proj = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.attention_layer = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=batch_first)\n",
    "        self.layernorm1 = nn.LayerNorm(seq_len, embed_dim)\n",
    "\n",
    "        # now we set out cross attention layer\n",
    "        self.cross_attention_layer = MultiHeadCrossAttention(d_model=embed_dim, num_heads=num_heads, batch_first=batch_first)\n",
    "        self.layernorm2 = nn.LayerNorm(seq_len, embed_dim)\n",
    "\n",
    "        # now our feed forward mechanism\n",
    "        self.feed_forward_layer = FeedForwardLayer(d_model=embed_dim, n_hidden=embed_dim * 4)\n",
    "        self.layernorm3 = nn.LayerNorm(seq_len, embed_dim)\n",
    "\n",
    "    def forward(self, trgt, trgt_mask, encoder_output):\n",
    "\n",
    "        # 1. we will projecy our initial qkv which is for the target variables and ouputs\n",
    "        qkv = self.qkv_proj(trgt)\n",
    "        q,k,v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # 2. feed out trgt and trgt mask into our multihead attention layer\n",
    "        # we will name is v because it turns into the values into our cross-attention layer\n",
    "        v_output, _ = self.attention_layer(query=q, key=k, value=v, attn_mask=trgt_mask)\n",
    "        v_output = self.layernorm1(v_output + trgt)\n",
    "\n",
    "        # 3. perform cross-attention\n",
    "        cross_output = self.cross_attention_layer(encoder_output, v_output, trgt_mask)\n",
    "        cross_output = self.layernorm2(cross_output + v_output)\n",
    "\n",
    "        # 4. feed into our feed-forward module\n",
    "        x = self.feed_forward_layer(cross_output)\n",
    "        return self.layernorm3(x + cross_output)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, seq_len, embed_dim, num_heads = 8, num_blocks = 12):\n",
    "        super().__init__()\n",
    "        self.seq_len, self.embed_dim, self.num_heads, self.num_blocks = seq_len, embed_dim, num_heads, num_blocks\n",
    "        self.sequential_decoder = nn.Sequential(\n",
    "            *[DecoderBlock(seq_len=seq_len, embed_dim=embed_dim, num_heads=num_heads)\n",
    "            for i in range(num_blocks)])\n",
    "        \n",
    "    def forward(self, trgt, trgt_mask, encoder_output):\n",
    "        return self.sequential_decoder(trgt, trgt_mask, encoder_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coursera",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
