{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Attention From Scratch Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting our values to explore self attention\n",
    "# we begin with the embedding dimension\n",
    "embed_dim = 512\n",
    "\n",
    "l = 2000 # length of the sequence\n",
    "\n",
    "# dimension of the embddings\n",
    "d_k = embed_dim\n",
    "d_v = embed_dim\n",
    "d_q = embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our linear layers using numpy, think of these as our weights\n",
    "# realistically these values will be adjusted as they are weighted parameters\n",
    "query = np.random.randn(l, d_q)\n",
    "key = np.random.randn(l, d_k)\n",
    "value = np.random.randn(l, d_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 512), (2000, 512), (2000, 512))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape, key.shape, value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the variance and std of each of our weights\n",
    "def stats(name, x, fn): print(f\"{name} {fn.__name__}: {fn(x)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query mean: 0.001069653035030377\n",
      "query var: 1.0010328363909236\n"
     ]
    }
   ],
   "source": [
    "stats(\"query\", query, np.mean)\n",
    "stats(\"query\", query, np.var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so now lets actually calculate self-attention\n",
    "\n",
    "# first we will matrix multiply the query and keys - think of this as a lookup\n",
    "attn = np.matmul(query, key.T)\n",
    "\n",
    "# this should give us a (embedded_dim, embedded_dim) sized matrix\n",
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention-pre mean: 0.0019910089655932645\n",
      "attention-pre var: 513.2093028039534\n"
     ]
    }
   ],
   "source": [
    "# next thing we need to do is to scale down the values\n",
    "# as we can see, the stats of this matrix don't represent the initial stats. Which could through off the NN\n",
    "stats(\"attention-pre\", attn, np.mean)\n",
    "stats(\"attention-pre\", attn, np.var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so we scale it down by the embedding dimension size of we initialized earlier\n",
    "attn_normalized =  attn / math.sqrt(d_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention-post mean: 8.799099631088812e-05\n",
      "attention-post var: 1.0023619195389712\n"
     ]
    }
   ],
   "source": [
    "# this should give us better stats\n",
    "stats(\"attention-post\", attn_normalized, np.mean)\n",
    "stats(\"attention-post\", attn_normalized, np.var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2000)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# masking\n",
    "# this ensures words dont get context from words genereated in the future\n",
    "# this is very important in the decoder step as this would be cheating for the neural net\n",
    "# however, this isnt very important for the encoder step\n",
    "\n",
    "\n",
    "# lets create a mask\n",
    "mask = np.tril(np.ones((l, l)))\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 0., 0.],\n",
       "       [1., 1., 1., ..., 1., 1., 0.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to give an idea of what the mask looks like. \n",
    "# for each sample, it will turn the future 'word' or 'token' as a 0 so it will not compute it\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, ..., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, ..., -inf, -inf, -inf],\n",
       "       [  0.,   0.,   0., ..., -inf, -inf, -inf],\n",
       "       ...,\n",
       "       [  0.,   0.,   0., ...,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., ...,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0., ...,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we will turn the 0s into -inf\n",
    "# and all the ones a zero\n",
    "mask[mask == 0] = -np.infty\n",
    "mask[mask == 1] = 0\n",
    "\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.36844932,        -inf,        -inf, ...,        -inf,\n",
       "               -inf,        -inf],\n",
       "       [-0.32793882, -0.32327196,        -inf, ...,        -inf,\n",
       "               -inf,        -inf],\n",
       "       [ 1.31093826, -0.67981886,  0.96435411, ...,        -inf,\n",
       "               -inf,        -inf],\n",
       "       ...,\n",
       "       [-0.13647658,  1.28365034, -0.00917514, ...,  1.31312584,\n",
       "               -inf,        -inf],\n",
       "       [ 1.11890654, -0.68432322,  0.07081183, ..., -0.27636055,\n",
       "         0.71917982,        -inf],\n",
       "       [ 2.43944293,  0.52733486,  1.09641017, ..., -0.61578768,\n",
       "         0.51607829,  1.05108062]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# why do we want to do this?\n",
    "# the reason is, we are adding the mask to the attention calculation. So if we kept the one, this will shift of values by one which\n",
    "# will skew the data and we dont want this\n",
    "attn_normalized + mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we will be applying a softmax to get our attention matrix\n",
    "def softmax(x): return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [4.98833286e-01, 5.01166714e-01, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       [5.42390154e-01, 7.40861396e-02, 3.83523707e-01, ...,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [2.43633168e-04, 1.00806770e-03, 2.76708644e-04, ...,\n",
       "        1.03822324e-03, 0.00000000e+00, 0.00000000e+00],\n",
       "       [9.04626776e-04, 1.49051621e-04, 3.17166775e-04, ...,\n",
       "        2.24136529e-04, 6.06555199e-04, 0.00000000e+00],\n",
       "       [3.74031637e-03, 5.52701134e-04, 9.76419895e-04, ...,\n",
       "        1.76213242e-04, 5.46514501e-04, 9.33147392e-04]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_matrix = softmax(attn_normalized + mask)\n",
    "attention_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets now just write a simple function that calculates self attention\n",
    "# remember, we will only want the mask in the decoder and not the encoder\n",
    "def self_attention(query, key, value, mask=None):\n",
    "    scaled = np.matmul(query, key.T) / math.sqrt(query.shape[-1])\n",
    "    if mask: scaled = scaled + mask\n",
    "    attention_matrix = softmax(scaled)\n",
    "    out = np.matmul(attention_matrix, value)\n",
    "    return attention_matrix, out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coursera",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
