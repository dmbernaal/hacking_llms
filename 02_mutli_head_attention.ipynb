{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multihead Atttention From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why multihead?\n",
    "In simple terms, think of each attention head as having its own set of 'glasses' to look at the data. Each set of glasses allows a head to see specific features or relationships in the data. By having multiple heads (multiple sets of glasses), the model can see and understand the data from various perspectives at the same time. This multiplicity of perspectives helps in creating a more comprehensive representation of data. Which is crucial for tasks that require a nuanced understanding of complex relationships, like language undertstanding and translation. \n",
    "\n",
    "The use of multiple attention heads in Transformers models enhances the networks ability to capture various types of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length = 4\n",
    "batch_size = 1\n",
    "input_dim = 512\n",
    "d_model = 512 # the output dimension size. This should match the input dimension so we can stack\n",
    "\n",
    "# initialize a random tensor\n",
    "x = torch.randn((batch_size, sequence_length, input_dim))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now instead of creating seperate qkl layers we will create it at once\n",
    "qkv_layer = nn.Linear(input_dim, 3 * d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1536])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so the first step is to feed our x into our qkv_layer\n",
    "qkv = qkv_layer(x)\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now lets define the number of heads\n",
    "num_heads = 8\n",
    "head_dim = d_model // num_heads # the dimension is just splitting the dimension of the model\n",
    "head_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 192])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to apply the multiple heads we simply just want to reshape our qkv output\n",
    "qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3 * head_dim)\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as you see this is just reshaping the weighted matrix, thats it\n",
    "4 * 8 * 192 == 4 * 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 192])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next we just want to permute the match (switch dimensions)\n",
    "# this will turn our dimensions from: [batch_size, sequence_length, num_heads, 3*head_dim]\n",
    "#                                 to: [batch_size, num_heads, sequence_length, 3*head_dim]\n",
    "qkv = qkv.permute(0, 2, 1, 3)\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then we will break down the layers individually by the last dimension\n",
    "q, k, v = qkv.chunk(3, dim=-1)\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we simply perform our self attention as we've done before\n",
    "# we only want to transpose the last two dimensions\n",
    "scaled = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(q.size(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember this is just attention so in this case we should get the last dimensions as 4x4 \n",
    "# as its performing an attention matrix on the sequence\n",
    "scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we will do the same thing with masks\n",
    "# remember, that we will only use the masks for the decoder and not the encoder\n",
    "mask = torch.full(scaled.size(), float(\"-inf\")) # create a matrix of all inf with size of the scaled matrix\n",
    "mask = torch.triu(mask, diagonal=1) # fill the lower diagn with 0\n",
    "mask[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2898,    -inf,    -inf,    -inf],\n",
       "        [-0.3619,  0.4547,    -inf,    -inf],\n",
       "        [-0.1549,  0.0713, -0.1431,    -inf],\n",
       "        [ 0.0760, -0.1361,  0.3425,  0.2237]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# then for the instance we are adding the mask we simply just add\n",
    "scaled += mask\n",
    "scaled[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lastly to calculate our attention we need to use softmax\n",
    "attention = F.softmax(scaled, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 64])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lastly, we calculate the value\n",
    "value = torch.matmul(attention, v)\n",
    "value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets put it all into a class in this case, not a function\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    scaled = torch.matmul(q, k.tranpose(-1, -2)) / math.sqrt(q.size(-1))\n",
    "    if mask: scaled += mask\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return attention, values\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, n_heads = 8):\n",
    "        super(MultiHeadAttention).__init__()\n",
    "        self.input_dim, self.embed_dim, self.n_heads = input_dim, embed_dim, n_heads\n",
    "        self.head_dim = embed_dim // n_heads\n",
    "\n",
    "        self.qkv = nn.Linear(input_dim, 3*embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        \"\"\"x: [batch_size, sequence_length, embedded_dim]\"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # [batch_size, seq_length, embed_dim*3]\n",
    "        qkv = self.qkv(x)\n",
    "\n",
    "        # [batch_size, seq_length, n_heads, 3 * head_dim]\n",
    "        qkv = qkv.reshape(batch_size, seq_len, self.n_heads, 3*self.head_dim)\n",
    "\n",
    "        # [batch_size, n_heads, seq_length, 3 * head_dim]\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "\n",
    "        # chunk to get the qkv layer separate\n",
    "        # [batch_size, n_heads, seq_length, head_dim]\n",
    "        q,k,v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # calculate attention and values\n",
    "        attention, values = scaled_dot_product(q, k, v, mask)\n",
    "\n",
    "        # reshape the valuess matrix back to original\n",
    "        # remember: n_heads = 8\n",
    "        #         : head_dim = 64\n",
    "        #         : n_heads * head_dim = 512, which is the same size as the input\n",
    "        values = values.permute(0, 2, 1, 3)\n",
    "        values = values.reshape(batch_size, seq_len, self.embed_dim)\n",
    "\n",
    "        # feed our values through our linear projectioj\n",
    "        output = self.o_proj(values)\n",
    "        return attention, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, n_heads = 8):\n",
    "        super(MultiHeadAttention).__init__()\n",
    "        self.input_dim, self.embed_dim, self.n_heads = input_dim, embed_dim, n_heads\n",
    "        self.head_dim = embed_dim // n_heads\n",
    "        self.qkv = nn.Linear(input_dim, 3*embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.reshape(batch_size, seq_len, self.n_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        q,k,v = qkv.chunk(3, dim=-1)\n",
    "        attention, values = scaled_dot_product(q, k, v, mask)\n",
    "        values = values.permute(0, 2, 1, 3)\n",
    "        values = values.reshape(batch_size, seq_len, self.embed_dim)\n",
    "        output = self.o_proj(values)\n",
    "        return attention, output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coursera",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
